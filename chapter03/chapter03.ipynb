{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "## Finite Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Agent-Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as diferent from each other as possible. The framework is abstract and flexible and can be applied in many diferent ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "- ***Inventory Management in Retail***\n",
    "\n",
    "  - ***States: current inventory levels of various products in a retail store***\n",
    "  - ***Actions: restocking, discounting, or leaving inventory unchanged***\n",
    "  - ***Rewards: might be based on sales revenue, with positive rewards for selling products at full price and negative rewards for holding excess inventory or discounting heavily***\n",
    "\n",
    "- ***Robot Navigation in a Maze***\n",
    "\n",
    "  - ***States: robot's position within a maze***\n",
    "  - ***Actions: the robot's movement directions (e.g., up, down, left, right)***\n",
    "  - ***Rewards: positive reward for reaching the goal state, negative reward for hitting obstacles or taking longer paths***\n",
    "\n",
    "- ***Healthcare Treatment Planning***\n",
    "\n",
    "  - ***States: current health condition of a patient (e.g., vital signs, symptoms, test results)***\n",
    "  - ***Actions: prescribing medication, ordering tests, or recommending lifestyle changes***\n",
    "  - ***Rewards: could be based on patient outcomes, such as improved health or reduced symptoms, with penalties for adverse effects of treatments or worsening conditions. Additionally, rewards may also consider cost-effectiveness of treatments***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "- ***An exception could be a task where the transition probabilities not only depends on the actual state, but previous states, breaking the Markov property. An example could be weather forecast, where the likekihood of the next weather conditions depends on preceding conditions***\n",
    "- ***Another exception can occur when the agent cannot fully identify a state inside the environment (partial observability), altough they can be converted to MDPs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3\n",
    "Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "- ***There are various factors to consider where to draw the line between the agent and the environment: task complexity, observation space, action space, computational efficienty, etc.***\n",
    "- ***There is no a \"better\" location over the others. The choice of the level of abstraction is a trade-off between complexity and generalization***\n",
    "- ***It is also possible to combine two or more levels of abstraction in a hierarchical learning system***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4\n",
    "Give a table analogous to that in Example 3.3, but for $p(s', r|s, a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s', r | s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0$.\n",
    "\n",
    "- ***Similar to Example 3.3 because rewards are deterministic given $s$, $a$ and $s'$***\n",
    "\n",
    "|  $s$  |    $a$    | $s'$ |      $r$      | $p(s', r \\| s, a)$ |\n",
    "|-------|-----------|------|---------------|--------------------|\n",
    "| high  | search    | high | $r_{search}$  | $\\alpha$           |\n",
    "| high  | search    | low  | $r_{search}$  | $1 - \\alpha$       |\n",
    "| low   | search    | high | $-3$          | $1 - \\beta$        |\n",
    "| low   | search    | low  | $r_{search}$  | $\\beta$            |\n",
    "| high  | wait      | high | $r_{waig}$    | $1$                |\n",
    "| low   | wait      | low  | $r_{wait}$    | $1$                |\n",
    "| low   | recharge  | high | $0$           | $1$                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.5\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "- $\\sum_{s'\\in S^+} \\sum_{r\\in R} p(s', r | s, a) = 1$ for all $s\\in S^+$, $a\\in A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.6\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $−1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted,  continuing formulation of this task?\n",
    "\n",
    "- $G_t = - \\gamma^{T-t-1}$ for all $t < T$, with $G_T = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.7\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you efectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "- ***The agent will receive a sparse reward, $+1$ independently of the time taken to solve the maze, so it does not have a notion of the time taken to reach the goal. Thus, the agent will not improve as it does not have a way to differentiante between short and long runs. A way of solving this is to add a discounting factor to the return***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.8\n",
    "Suppose $\\gamma$ = 0.5 and the following sequence of rewards is received $R_1 = −1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, $...$, $G_5$? Hint: Work backwards.\n",
    "\n",
    "- $G_5 = 0$\n",
    "- $G_4 = R_5 + \\gamma G_5 = 2$\n",
    "- $G_3 = R_4 + \\gamma G_4 = 4$\n",
    "- $G_2 = R_3 + \\gamma G_3 = 8$\n",
    "- $G_1 = R_2 + \\gamma G_2 = 6$\n",
    "- $G_0 = R_1 + \\gamma G_1 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.9\n",
    "Suppose $\\gamma$ = 0.9 and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$ s. What are $G_1$ and $G_0$?\n",
    "\n",
    "- $G_1 = \\sum_{k=0}^{\\infty} \\gamma^k R_{k+2} = 7 * \\frac{1}{1-0.9} = 70$\n",
    "- $G_0 = R_1 + \\gamma G_1 = 65$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.10\n",
    "Prove the second equality in (3.10).\n",
    "\n",
    "- $G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\sum_{k=-1}^{\\infty} \\gamma^{k+1} = 1 + \\sum_{k=0}^{\\infty} \\gamma^{k+1}$\n",
    "- $\\gamma G_t = \\sum_{k=0}^{\\infty} \\gamma^{k+1}$\n",
    "- $\\gamma G_t - G_t = 1 + \\sum_{k=0}^{\\infty} \\gamma^{k+1} - \\sum_{k=0}^{\\infty} \\gamma^{k+1} = 1$\n",
    "- $G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.11\n",
    "If the current state is $S_t$, and actions are selected according to a stochastic policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function $p$ (3.2)?\n",
    "\n",
    "- $\\mathbb{E}[R_{t+1} | S_t = s] = \\sum_{a} \\pi(a | s) * \\mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \\sum_{a} \\pi(a | s) \\sum_{r} r \\sum_{s'} p(s', r | s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.12\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.13\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument $p$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlintro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
