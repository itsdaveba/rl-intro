{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "## Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Action-value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon = 0.5$, what is\n",
    "the probability that the greedy action is selected?\n",
    "\n",
    "- ***0.75***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bandit import run_episode\n",
    "from agents import EpsilonGreedyAgent\n",
    "\n",
    "\n",
    "k = 10\n",
    "seed = 42\n",
    "num_envs = 2000\n",
    "\n",
    "vector_kwargs = dict(k=k)\n",
    "env = gym.make_vec(\"ArmedBandit-v0\", num_envs, \"custom\", vector_kwargs)\n",
    "env.reset(seed=seed)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"$\\epsilon$ = 0 (greedy)\": EpsilonGreedyAgent(k, 0.0, num_envs),\n",
    "    \"$\\epsilon$ = 0.01\": EpsilonGreedyAgent(k, 0.01, num_envs),\n",
    "    \"$\\epsilon$ = 0.1\": EpsilonGreedyAgent(k, 0.1, num_envs)\n",
    "}\n",
    "\n",
    "data = {\"avg_reward\": {}, \"optimal_action\": {}}\n",
    "for i, (key, agent) in enumerate(agents.items()):\n",
    "    rewards, optimals = run_episode(env, agent, seed + i)\n",
    "    data[\"avg_reward\"][key] = rewards.mean(axis=1)\n",
    "    data[\"optimal_action\"][key] = 100 * optimals.mean(axis=1)\n",
    "\n",
    "\n",
    "plt.figure(layout=\"constrained\")\n",
    "\n",
    "plt.subplot(211)\n",
    "for key in agents.keys():\n",
    "    plt.plot(key, data=data[\"avg_reward\"])\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(212)\n",
    "for key in agents.keys():\n",
    "    plt.plot(key, data=data[\"optimal_action\"])\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal action\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Bandit example\n",
    "Consider a $k$-armed bandit problem with $k = 4$ actions, denoted $1$, $2$, $3$, and $4$. Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1$, $R_1 = −1$, $A_2 = 2$, $R_2 = 1$, $A_3 = 2$, $R_3 = −2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "- ***Definitely occured on $t = 4$ and $t = 5$***\n",
    "\n",
    "- ***Possibly have ocurred on $t = 1$, $t = 2$ and $t = 3$***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "- ***$\\epsilon = 0.01$ will perform best in the long run***\n",
    "- ***In the long run, the probability of selecting the best action will be $(1 - \\epsilon) + \\epsilon / k = 1 - (k - 1) \\epsilon / k$, which indicates that the lower the $\\epsilon$, the greater the probability***\n",
    "- ***% Optimal action: $99.1\\%$***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlintro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
