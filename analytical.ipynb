{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import chapter03.gridworld\n",
    "import chapter04.gridworld\n",
    "import chapter04.car_rental\n",
    "import chapter04.gambler\n",
    "\n",
    "id = \"GridWorld-v0\"\n",
    "\n",
    "if id == \"GridWorld-v0\":\n",
    "    exceptional_reward_dynamics = {\n",
    "        \"A\": {\"from\": (0, 1), \"to\": (4, 1), \"reward\": 10.0},\n",
    "        \"B\": {\"from\": (0, 3), \"to\": (2, 3), \"reward\": 5.0}\n",
    "    }\n",
    "    grid = True\n",
    "    env = gym.make(id, shape=(5, 5), reward_dynamics=exceptional_reward_dynamics)\n",
    "    policy = 0.25 * np.ones(tuple(env.observation_space.nvec) + (env.action_space.n,), dtype=np.float32)\n",
    "    action_map = [\"right\", \"up\", \"left\", \"down\"]\n",
    "    gamma = 0.9\n",
    "    val_text = True\n",
    "    plot_pol = False\n",
    "elif id == \"GridWorld-v1\":\n",
    "    grid = True\n",
    "    terminal_states = ((0, 0), (3, 3))\n",
    "    env = gym.make(id, shape=(4, 4), terminal_states=terminal_states)\n",
    "    policy = 0.25 * np.ones(tuple(env.observation_space.nvec) + (env.action_space.n,), dtype=np.float32)\n",
    "    action_map = [\"right\", \"up\", \"left\", \"down\"]\n",
    "    gamma = 1.0\n",
    "    val_text = True\n",
    "    plot_pol = False\n",
    "elif id.startswith(\"CarRental\"):\n",
    "    grid = True\n",
    "    if id == \"CarRental-v0\":\n",
    "        env = gym.make(\"CarRental-v0\")\n",
    "    elif id == \"CarRental-v1\":\n",
    "        env = gym.make(\"CarRental-v0\", modified=True)\n",
    "    policy = np.zeros(tuple(env.observation_space.nvec) + (env.action_space.n,), dtype=np.float32)\n",
    "    policy[:, :, 0] = 1.0\n",
    "    action_map = []\n",
    "    gamma = 0.9\n",
    "    val_text = False\n",
    "    plot_pol = True\n",
    "elif id.startswith(\"Gambler\"):\n",
    "    grid = False\n",
    "    if id == \"Gambler-v0\":\n",
    "        env = gym.make(\"Gambler-v0\", prob_heads=0.40)\n",
    "    elif id == \"Gambler-v1\":\n",
    "        env = gym.make(\"Gambler-v0\", prob_heads=0.25)\n",
    "    elif id == \"Gambler-v2\":\n",
    "        env = gym.make(\"Gambler-v0\", prob_heads=0.55)\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n), dtype=np.float32)\n",
    "    policy[:, 0] = 1.0\n",
    "    action_map = [i + 1 for i in range(env.action_space.n)]\n",
    "    gamma = 1.0\n",
    "    val_text = False\n",
    "    plot_pol = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.choice(env.action_space.n, p=policy[tuple(state) if grid else state].flatten())\n",
    "if id.startswith(\"CarRental\"):\n",
    "    action -= env.action_space.n * (action // (-env.action_space.start + 1))\n",
    "state, reward, terminated, _, info = env.step(action)\n",
    "print(\"Action:\", action_map[action] if action_map else action)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Terminated:\", terminated)\n",
    "print(\"Info:\", info)\n",
    "print(\"State:\", state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.nvec.prod() if grid else env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "A = env.unwrapped.prob.reshape(num_states, num_actions, num_states)\n",
    "b = (A * env.unwrapped.rewards.reshape(num_states, num_actions, num_states)).sum(axis=2, keepdims=True)\n",
    "\n",
    "if hasattr(env.unwrapped, \"terminal_states\"):\n",
    "    if grid:\n",
    "        terminal = np.arange(num_states).reshape(env.observation_space.nvec)[tuple(zip(*env.unwrapped.terminal_states))]\n",
    "    else:\n",
    "        terminal = env.unwrapped.terminal_states\n",
    "else:\n",
    "    terminal = []\n",
    "diagonal = np.diag(np.logical_not(np.logical_or.reduce(np.eye(num_states)[terminal])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Policy Evaluation\n",
    "    pol_eval = policy.reshape(num_states, 1, num_actions)\n",
    "    A_eval = diagonal - gamma * (pol_eval @ A).squeeze()\n",
    "    b_eval = (pol_eval @ b).squeeze()\n",
    "    v_eval = np.linalg.solve(A_eval, b_eval)\n",
    "\n",
    "    # Policy Improvement\n",
    "    old_policy = policy.copy()\n",
    "    A_impr = gamma * A @ v_eval + b.squeeze()\n",
    "    if id in [\"Gambler-v0\", \"Gambler-v1\"]:\n",
    "        A_impr = np.round(A_impr, decimals=6)\n",
    "    argmax = A_impr == np.max(A_impr, axis=1, keepdims=True)\n",
    "    policy = (argmax / np.sum(argmax, axis=1, keepdims=True))\n",
    "    if grid:\n",
    "        policy = policy.reshape((*env.observation_space.nvec, num_actions))\n",
    "    if not np.any(policy - old_policy):\n",
    "        break\n",
    "\n",
    "# Plot Value Function\n",
    "v = v_eval.reshape(env.observation_space.nvec) if grid else v_eval\n",
    "if grid:\n",
    "    if val_text:\n",
    "        plt.imshow(v, cmap=\"winter\")\n",
    "        for state in product(*[range(i) for i in v.shape]):\n",
    "            plt.text(*state[::-1], f\"{v[state]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
    "    else:\n",
    "        plt.imshow(v, cmap=\"plasma\")\n",
    "    plt.title(\"Optimal state-value function\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.plot(v)\n",
    "    plt.title(\"Optimal state-value function\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if plot_pol:\n",
    "    # Plot Optimal Policy\n",
    "    if grid:\n",
    "        deterministic = np.argmax(policy, axis=-1)\n",
    "        deterministic -= env.action_space.n * (deterministic // (-env.action_space.start + 1))\n",
    "        plt.imshow(deterministic, cmap=\"plasma\", vmin=env.action_space.start, vmax=-env.action_space.start)\n",
    "        plt.title(\"Optimal policy\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    else:\n",
    "        deterministic = np.argmax(policy, axis=-1) + 1\n",
    "        plt.step(np.arange(env.observation_space.n), deterministic, where=\"mid\")\n",
    "        plt.title(\"Optimal policy\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_iter = np.zeros(num_states)\n",
    "theta = 0.0001\n",
    "\n",
    "# Value Iteration\n",
    "while True:\n",
    "    old_v = v_iter.copy()\n",
    "    v_iter = np.max(gamma * A @ v_iter + b.squeeze(), axis=1)\n",
    "    if max(abs(v_iter - old_v)) < theta:\n",
    "        break\n",
    "    \n",
    "# Optimal Policy\n",
    "A_impr = gamma * A @ v_iter + b.squeeze()\n",
    "if id in [\"Gambler-v0\"]:\n",
    "    A_impr = np.round(A_impr, decimals=6)\n",
    "argmax = A_impr == np.max(A_impr, axis=1, keepdims=True)\n",
    "policy = (argmax / np.sum(argmax, axis=1, keepdims=True))\n",
    "if grid:\n",
    "    policy = policy.reshape((*env.observation_space.nvec, num_actions))\n",
    "\n",
    "# Plot Value Function\n",
    "v = v_iter.reshape(env.observation_space.nvec) if grid else v_iter\n",
    "if grid:\n",
    "    if val_text:\n",
    "        plt.imshow(v, cmap=\"winter\")\n",
    "        for state in product(*[range(i) for i in v.shape]):\n",
    "            plt.text(*state[::-1], f\"{v[state]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
    "    else:\n",
    "        plt.imshow(v, cmap=\"plasma\")\n",
    "    plt.title(\"Optimal state-value function\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.plot(v)\n",
    "    plt.title(\"Optimal state-value function\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if plot_pol:\n",
    "    # Plot Optimal Policy\n",
    "    if grid:\n",
    "        deterministic = np.argmax(policy, axis=-1)\n",
    "        deterministic -= env.action_space.n * (deterministic // (-env.action_space.start + 1))\n",
    "        plt.imshow(deterministic, cmap=\"plasma\", vmin=env.action_space.start, vmax=-env.action_space.start)\n",
    "        plt.title(\"Optimal policy\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    else:\n",
    "        deterministic = np.argmax(policy, axis=-1) + 1\n",
    "        plt.step(np.arange(env.observation_space.n), deterministic, where=\"mid\")\n",
    "        plt.title(\"Optimal policy\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlintro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
